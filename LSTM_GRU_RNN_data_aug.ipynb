{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonathanCristovao/LSTM_GRU_RNN_human-activity-recognition/blob/main/LSTM_GRU_RNN_data_aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16WhqkwcseIu"
      },
      "source": [
        "# This notebook was run on google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pcRQvKt9r4Q",
        "outputId": "92b264c1-4e90-4518-cd2a-93db6950f85e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csFMJZEe9I0I"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai6fqg4x9rM9",
        "outputId": "84569e54-8baf-479d-f449-c0ac35a9f416"
      },
      "outputs": [],
      "source": [
        "!pip install keras_self_attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_52yqQ6hc560"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from scipy import stats\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "import glob\n",
        "import csv\n",
        "import os\n",
        "%matplotlib inline\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-ak4Az0j11p"
      },
      "source": [
        "## The dataset used in this notebook can be downloaded here:\n",
        "[Click here to download the Dataset](https://www.kaggle.com/datasets/jonathansilva2020/dataset-for-human-activity-recognition-har)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CZofzgJ9I0K"
      },
      "outputs": [],
      "source": [
        "interesting_dir = \"/content/drive/MyDrive/dataset_aug_HAR\"\n",
        "interesting_labels = ['seated','stand','walk','iw_uphill']\n",
        "save_filename = \"activity\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tddFIoe9I0M"
      },
      "source": [
        "## Concate and label the raw data  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXYz5psi9I0N"
      },
      "outputs": [],
      "source": [
        "def data_concate_label(path,label):\n",
        "    all_files1 = glob.glob(path + \"/*.csv\")\n",
        "    \n",
        "    li = []\n",
        "\n",
        "    for filename in all_files1:\n",
        "        df = pd.read_csv(filename, index_col=None, header=0)\n",
        "        \n",
        "        if 'label' in df.columns:\n",
        "            pass\n",
        "        else:df['label'] = str(label)\n",
        "        li.append(df)\n",
        "    frame = pd.concat(li, axis=0, ignore_index=True)\n",
        "    if len(df.columns) == 7:#8 :\n",
        "        new_header =['time','ID_S','Q_I','Q_J','Q_K','Q_R','label']\n",
        "        #new_header =['date','time','ID_S','Q_I','Q_J','Q_K','Q_R','label']\n",
        "    \n",
        "    else :\n",
        "       \n",
        "        #new_header =['date','time','ID_S','Q_I','Q_J','Q_K','Q_R','label']\n",
        "        new_header =['time','ID_S','Q_I','Q_J','Q_K','Q_R','label']\n",
        "\n",
        "    frame = frame[:160000]#frame[:16000]\n",
        "    \n",
        "    print(frame.shape)\n",
        "    print(len(df.columns))\n",
        "    frame.to_csv(\"/content/drive/MyDrive/dataset_aug_HAR/activity_\"+str(label)+\".csv\",index = False,header=new_header)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N7NLf7KyLjs",
        "outputId": "d2c679ae-f222-4058-9670-cd046608dfa4"
      },
      "outputs": [],
      "source": [
        "path1 = \"/content/drive/MyDrive/dataset_aug_HAR/seated\"\n",
        "label1 = \"seated\"\n",
        "path2 = \"/content/drive/MyDrive/dataset_aug_HAR/stand\"\n",
        "label2 = \"stand\"\n",
        "path3 = \"/content/drive/MyDrive/dataset_aug_HAR/walk\"\n",
        "label3 = \"walk\"\n",
        "path4 = \"/content/drive/MyDrive/dataset_aug_HAR/wuphill\"\n",
        "label4 = \"iw_uphill\"\n",
        "data_concate_label(path1,label1)\n",
        "data_concate_label(path2,label2)\n",
        "data_concate_label(path3,label3)\n",
        "data_concate_label(path4,label4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGWpUbXX9I0P"
      },
      "source": [
        "## Concate all data into one .csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "3jfl5sIf9I0Q",
        "outputId": "a119682b-8dc0-4d15-8c8e-9054db026424",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "df = pd.DataFrame()\n",
        "for i in range(len(interesting_labels)):\n",
        "    df_tmp = pd.read_csv(interesting_dir+\"/\"+save_filename+\"_\"+interesting_labels[i]+'.csv',header = 0)\n",
        "    df = pd.concat([df, df_tmp])\n",
        "print(len(df.columns))\n",
        "\n",
        "drop_list = ['time']#['date','time']\n",
        "\n",
        "df = df.drop(drop_list, axis=1)\n",
        "df = df.dropna()\n",
        "\n",
        "all_data_filename = \"total.csv\"\n",
        "df.to_csv(interesting_dir+all_data_filename)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIVWZ7qF9I0S"
      },
      "source": [
        "## Feature correlation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6lqHghB9I0S",
        "outputId": "3a3743a8-c873-4b17-e656-5e278c22b7be",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_cor = copy.deepcopy(df) \n",
        "\n",
        "features = df_cor.columns[0:-1]\n",
        "df_cor[\"label\"] = pd.factorize(df_cor.label)[0]\n",
        "traindf = pd.DataFrame(df_cor,columns=features)\n",
        "\n",
        "feature_name = list(traindf.columns)\n",
        "print(feature_name)\n",
        "y = df_cor['label']\n",
        "X = traindf.copy()\n",
        "X.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "hK9Au9z29I0T",
        "outputId": "a575f379-33bd-4741-85ae-48e96231fb34"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X,y)\n",
        "print(model.feature_importances_) \n",
        "#plot graph of feature importances for better visualization\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "feat_importances.nlargest(10).plot(kind='barh')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-HsSboK9I0U",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#get correlations of each features in dataset\n",
        "#corrmat = df_cor.corr()\n",
        "#top_corr_features = corrmat.index\n",
        "#plt.figure(figsize=(20,20))\n",
        "#plot heat map\n",
        "#g=sns.heatmap(df_cor[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMpkc4pQ9I0U",
        "outputId": "26748f5b-88cd-47c5-fb5f-2ca4979f3339"
      },
      "outputs": [],
      "source": [
        "num_feats= len(feature_name)\n",
        "X.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etbf339z9I0V",
        "outputId": "8d531af5-ba81-46f0-a1ba-e548182e290d"
      },
      "outputs": [],
      "source": [
        "def cor_selector(X, y,num_feats):\n",
        "    cor_list = []\n",
        "    feature_name = X.columns.tolist()\n",
        "    \n",
        "    # calculate the correlation with y for each feature\n",
        "    for i in X.columns.tolist():\n",
        "        cor = np.corrcoef(X[i], y)[0, 1]\n",
        "        cor_list.append(cor)\n",
        "        \n",
        "    # replace NaN with 0\n",
        "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
        "    \n",
        "    # feature name\n",
        "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
        "   \n",
        "    # feature selection? 0 for not select, 1 for select\n",
        "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
        "\n",
        "    return cor_support, cor_feature, cor_list\n",
        "\n",
        "cor_support, cor_feature, cor_list = cor_selector(X, y,num_feats)\n",
        "cor_val=[abs(x) for x in cor_list]\n",
        "cor_result = dict(zip(cor_feature, cor_val))\n",
        "myDict = {key:val for key, val in cor_result.items() if val > 0}\n",
        "{k: v for k, v in sorted(myDict.items(),reverse=True, key=lambda item: item[1])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS2rAhS99I0W",
        "outputId": "40b0ef06-a28f-4724-fc0b-09d0cbb6f326"
      },
      "outputs": [],
      "source": [
        "activities = df['label'].value_counts().index\n",
        "activities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huRwjGga9I0Z"
      },
      "source": [
        "## Split training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEqVzdL-9I0Z",
        "outputId": "c0c6371f-db71-4abb-8a16-2602c52fade9"
      },
      "outputs": [],
      "source": [
        "def train_test_split(label, ratio):\n",
        "    split_point = int(len(df[df.label == label]) * ratio)\n",
        "    return (df[df.label == label].iloc[:split_point, :], df[df.label == label].iloc[split_point:, :])\n",
        "\n",
        "split_ratio = 0.8\n",
        "train_data = pd.DataFrame([])\n",
        "test_data = pd.DataFrame([])\n",
        "\n",
        "for i in range(len(interesting_labels)):\n",
        "    (train, test) = train_test_split(interesting_labels[i], split_ratio)\n",
        "    train_data = pd.concat([train_data, train])\n",
        "    test_data = pd.concat([test_data, test])\n",
        "\n",
        "print(\"Number of train samples: \", len(train_data))\n",
        "print(\"Number of test samples: \", len(test_data))\n",
        "\n",
        "\n",
        "train_label = train_data['label'].to_frame()\n",
        "test_label = test_data['label'].to_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZCSA5xL9I0Z"
      },
      "source": [
        "## Data Segmentation (Sliding Window)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "err6EfHS9I0Z"
      },
      "outputs": [],
      "source": [
        "N_TIME_STEPS = 5 #sliding window length # 10 #120\n",
        "STEP = 2 # Sliding window step size 20\n",
        "N_FEATURES = 5\n",
        "\n",
        "def generate_sequence(x, y, n_time_steps, step):\n",
        "    \n",
        "    segments = []\n",
        "    labels = []\n",
        "    for i in range(0, len(x) - n_time_steps, step):\n",
        "\n",
        "        ID_S = x['ID_S'].values[i: i + n_time_steps]\n",
        "        Q_I = x['Q_I'].values[i: i + n_time_steps]\n",
        "        Q_J = x['Q_J'].values[i: i + n_time_steps]\n",
        "        Q_K = x['Q_K'].values[i: i + n_time_steps]\n",
        "        Q_R = x['Q_R'].values[i: i + n_time_steps]\n",
        "     \n",
        "        label = stats.mode(y['label'][i: i + n_time_steps])[0][0]\n",
        "        segments.append([ID_S, Q_I, Q_J, Q_K, Q_R])\n",
        "        labels.append(label)\n",
        "        \n",
        "    return segments, labels\n",
        "\n",
        "train_X, train_y = generate_sequence(train_data, train_label, N_TIME_STEPS, STEP)\n",
        "test_X, test_y = generate_sequence(test_data, test_label, N_TIME_STEPS, STEP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt-HNaqG9I0Z"
      },
      "outputs": [],
      "source": [
        "# reshape input segments and one-hot encode labels\n",
        "def reshape_segments(x, y, n_time_steps, n_features):\n",
        "    \n",
        "    x_reshaped = np.asarray(x, dtype= np.float32).reshape(-1, n_time_steps, n_features)\n",
        "    y_reshaped = np.asarray(pd.get_dummies(y), dtype = np.float32)\n",
        "    return x_reshaped, y_reshaped\n",
        "\n",
        "X_train, y_train = reshape_segments(train_X, train_y, N_TIME_STEPS, N_FEATURES)\n",
        "X_test, y_test = reshape_segments(test_X, test_y, N_TIME_STEPS, N_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJdwMt1L9I0a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjlqEMru9I0a"
      },
      "source": [
        "## Import DL Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFFl4sX99I0a"
      },
      "outputs": [],
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, LSTM, Flatten, Bidirectional\n",
        "from tensorflow.keras.layers import Attention\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from tensorflow.python.tools import freeze_graph\n",
        "from tensorflow.python.tools import optimize_for_inference_lib\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "N_CLASSES = 4\n",
        "L2 = 0.00001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYaKUb7w9I0b"
      },
      "source": [
        "## LSTM Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2ojrWNt9I0b",
        "outputId": "92e3d079-1a94-4a02-d38d-39d92da0b2ce",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# LSTM model\n",
        "from keras.backend import clear_session\n",
        "filepath = 'har_model_lstm_2021.h5'\n",
        "\n",
        "clear_session()\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(200, return_sequences=True,kernel_initializer='orthogonal',\n",
        "                             kernel_regularizer=l2(L2), recurrent_regularizer=l2(L2),\n",
        "                             bias_regularizer=l2(L2),name=\"LSTM_1\"),\n",
        "                             input_shape=(N_TIME_STEPS, N_FEATURES)))\n",
        "\n",
        "model.add(Flatten(name='Flatten'))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_3\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(N_CLASSES, activation='softmax', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_4\"))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "stop = EarlyStopping(monitor='val_loss', patience =5,\n",
        "                      verbose=0, mode='auto', baseline=None, \n",
        "                      restore_best_weights=False)\n",
        "reduce =  ReduceLROnPlateau(monitor='val_loss',\n",
        "                    factor=0.5,\n",
        "                    patience=2,\n",
        "                    verbose=1)\n",
        "callbacks = [checkpoint,stop]\n",
        "\n",
        "model.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "BATCH_SIZE =64\n",
        "N_EPOCHS = 300\n",
        "\n",
        "lstm = model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE, shuffle=True,epochs=N_EPOCHS,callbacks=callbacks,\n",
        "          validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "nsTDLchI9I0c",
        "outputId": "6ea667de-4d7e-4453-f680-86c51ec3a400",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# use LSTM model to predict\n",
        "\n",
        "y_pred_ohe = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred_ohe, axis=1)\n",
        "y_true_labels = np.argmax(y_test, axis=1)\n",
        "confusion_matrix = metrics.confusion_matrix(y_true=y_true_labels, y_pred=y_pred_labels)\n",
        "print(y_true_labels)\n",
        "LABELS = ['seated', 'stand','walk','iwuphill']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
        "plt.title(\"Confusion matrix - LSTM\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36L7EIPbLGa0",
        "outputId": "b3b632bd-94d1-4325-845a-2af8a0750197"
      },
      "outputs": [],
      "source": [
        "predIdxs = model.predict(X_test, batch_size=BATCH_SIZE)\n",
        "predIdxs = np.argmax(predIdxs, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wZSziZzKX2A",
        "outputId": "48a694e5-a93d-4fb2-bb6e-6101a351b935"
      },
      "outputs": [],
      "source": [
        "## show a classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=['seated', 'stand','walk','iwuphill']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "OaCxG9Un9I0c",
        "outputId": "52a63bf1-1e2e-4fd0-ec8d-04b51883e10e"
      },
      "outputs": [],
      "source": [
        "plt.plot(lstm.history['accuracy'])\n",
        "plt.plot(lstm.history['val_accuracy'])\n",
        "plt.title('model accuracy - LSTM')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylim(0,1)\n",
        "plt.legend(['train', 'val'], loc='best')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lstm.history['loss'])\n",
        "plt.plot(lstm.history['val_loss'])\n",
        "plt.title('model loss - LSTM')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez0TA6im9I0d"
      },
      "source": [
        "## Save the model into .pb file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE2brr6tuFHc"
      },
      "outputs": [],
      "source": [
        "# Save LSTM model\n",
        "\n",
        "#output_node_name = \"Dense_2/Softmax\"\n",
        "#MODEL_LSTM = str(N_TIME_STEPS)+\"_\"+str(STEP)+\"_\"+str(N_FEATURES)+\"_\"+str(N_CLASSES)+\"_nomagnew\"\n",
        "\n",
        "#tf.train.write_graph(K.get_session().graph_def, './models/LSTM/', \\     MODEL_LSTM + '_graph.pbtxt')\n",
        "#saver = tf.train.Saver()\n",
        "#saver.save(K.get_session(), './models/LSTM/' + MODEL_LSTM + '.chkp')\n",
        "\n",
        "#freeze_graph.freeze_graph('./models/LSTM/' +MODEL_LSTM+ '_graph.pbtxt', None, \\\n",
        "#   False, './models/LSTM/' + MODEL_LSTM+ '.chkp', output_node_name, \\\n",
        "#   \"save/restore_all\", \"save/Const:0\", \\\n",
        "#    './models/LSTM/' +MODEL_LSTM + '.pb', True, \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9nUUG5o5bZa"
      },
      "source": [
        "# GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeMwv8D95amw"
      },
      "outputs": [],
      "source": [
        "from keras.layers import GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IYZFvH55o7E",
        "outputId": "e53139ca-09b0-40d9-8b45-58b8e9d49132"
      },
      "outputs": [],
      "source": [
        "# GRU model\n",
        "from keras.backend import clear_session\n",
        "filepath = 'har_model_gru.h5'\n",
        "\n",
        "clear_session()\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(GRU(200, return_sequences=True,kernel_initializer='orthogonal',\n",
        "                             kernel_regularizer=l2(L2), recurrent_regularizer=l2(L2),\n",
        "                             bias_regularizer=l2(L2),name=\"LSTM_1\"),\n",
        "                             input_shape=(N_TIME_STEPS, N_FEATURES)))\n",
        "\n",
        "model.add(Flatten(name='Flatten'))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_3\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(N_CLASSES, activation='softmax', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_4\"))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "stop = EarlyStopping(monitor='val_loss', patience =5,\n",
        "                      verbose=0, mode='auto', baseline=None, \n",
        "                      restore_best_weights=False)\n",
        "reduce =  ReduceLROnPlateau(monitor='val_loss',\n",
        "                    factor=0.5,\n",
        "                    patience=2,\n",
        "                    verbose=1)\n",
        "callbacks = [checkpoint,stop]\n",
        "\n",
        "model.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "BATCH_SIZE =64\n",
        "N_EPOCHS = 300\n",
        "\n",
        "lstm = model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE, shuffle=True,epochs=N_EPOCHS,callbacks=callbacks,\n",
        "          validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "vfNbdSkj5y0k",
        "outputId": "23c40576-0ed3-465c-b713-d4faab046f18"
      },
      "outputs": [],
      "source": [
        "# GRU model to predict\n",
        "\n",
        "y_pred_ohe = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred_ohe, axis=1)\n",
        "y_true_labels = np.argmax(y_test, axis=1)\n",
        "confusion_matrix = metrics.confusion_matrix(y_true=y_true_labels, y_pred=y_pred_labels)\n",
        "print(y_true_labels)\n",
        "LABELS = ['seated', 'stand','walk','iwuphill']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
        "plt.title(\"Confusion matrix- GRU\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show();\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL_NREk_5328",
        "outputId": "0bb6e9fc-81bb-49cb-ea81-57d129350dec"
      },
      "outputs": [],
      "source": [
        "predIdxs = model.predict(X_test, batch_size=BATCH_SIZE)\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=['seated', 'stand','walk','iwuphill']))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "iUbge0eg6BYr",
        "outputId": "30e6fab2-e7eb-4cdc-e207-a4434cc0c324"
      },
      "outputs": [],
      "source": [
        "plt.plot(lstm.history['accuracy'])\n",
        "plt.plot(lstm.history['val_accuracy'])\n",
        "plt.title('model accuracy - GRU')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylim(0,1)\n",
        "plt.legend(['train', 'val'], loc='best')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lstm.history['loss'])\n",
        "plt.plot(lstm.history['val_loss'])\n",
        "plt.title('model loss - GRU')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='best')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pldt0IlV6LMY"
      },
      "source": [
        "RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmgQbt1k6eCY"
      },
      "outputs": [],
      "source": [
        "from keras.layers import SimpleRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2SzlYwV6i2d",
        "outputId": "99ee2e34-1709-4c4b-c866-db574b43973b"
      },
      "outputs": [],
      "source": [
        "# Simple RNN model\n",
        "from keras.backend import clear_session\n",
        "filepath = 'har_model_rnn.h5'\n",
        "\n",
        "clear_session()\n",
        "model = Sequential()\n",
        "\n",
        "model.add(SimpleRNN(200,input_shape=(N_TIME_STEPS, N_FEATURES), return_sequences=True,kernel_initializer='orthogonal',\n",
        "                             kernel_regularizer=l2(L2), recurrent_regularizer=l2(L2),\n",
        "                             bias_regularizer=l2(L2),))\n",
        "\n",
        "\n",
        "model.add(Flatten(name='Flatten'))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_3\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(N_CLASSES, activation='softmax', kernel_regularizer=l2(L2), bias_regularizer=l2(L2), name=\"Dense_4\"))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "stop = EarlyStopping(monitor='val_loss', patience =5,\n",
        "                      verbose=0, mode='auto', baseline=None, \n",
        "                      restore_best_weights=False)\n",
        "reduce =  ReduceLROnPlateau(monitor='val_loss',\n",
        "                    factor=0.5,\n",
        "                    patience=2,\n",
        "                    verbose=1)\n",
        "callbacks = [checkpoint,stop]\n",
        "\n",
        "model.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "BATCH_SIZE =64\n",
        "N_EPOCHS = 300\n",
        "\n",
        "lstm = model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE, shuffle=True,epochs=N_EPOCHS,callbacks=callbacks,\n",
        "          validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "-H777yvz6w5g",
        "outputId": "80a0be8e-24f7-49d1-9ac5-731670624312"
      },
      "outputs": [],
      "source": [
        "# RNN model to predict\n",
        "\n",
        "y_pred_ohe = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred_ohe, axis=1)\n",
        "y_true_labels = np.argmax(y_test, axis=1)\n",
        "confusion_matrix = metrics.confusion_matrix(y_true=y_true_labels, y_pred=y_pred_labels)\n",
        "print(y_true_labels)\n",
        "LABELS = ['seated', 'stand','walk','iwuphill']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
        "sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
        "plt.title(\"Confusion matrix - SimpleRNN\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcQc51bA-rNy",
        "outputId": "a034c8b8-1f97-4e74-cd17-c41abb7dac4b"
      },
      "outputs": [],
      "source": [
        "predIdxs = model.predict(X_test, batch_size=BATCH_SIZE)\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a classification report\n",
        "print(classification_report(y_test.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=['seated', 'stand','walk','iwuphill']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "mm5C3No96KXA",
        "outputId": "77db07c2-cfe4-44e8-cedb-e9bf494fbb4d"
      },
      "outputs": [],
      "source": [
        "plt.plot(lstm.history['accuracy'])\n",
        "plt.plot(lstm.history['val_accuracy'])\n",
        "plt.title('model accuracy - SimpleRNN')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylim(0,1)\n",
        "plt.legend(['train', 'val'], loc='best')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lstm.history['loss'])\n",
        "plt.plot(lstm.history['val_loss'])\n",
        "plt.title('model loss - SimpleRNN')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='best')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0a68ab044c6ea367198d7b58f0f8352272d5267d2d2c131306c104f6e9ede3d6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
